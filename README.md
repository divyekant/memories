# FAISS Memory - Local Semantic Memory for AI Coding Assistants

> Zero-cost, <50ms semantic search for Claude Code, Cursor, ChatGPT, and any AI that can make HTTP calls.

[![Status](https://img.shields.io/badge/status-production-green)]()
[![Docker](https://img.shields.io/badge/docker-ready-blue)]()
[![Cost](https://img.shields.io/badge/cost-%240%2Fmonth-brightgreen)]()
[![Speed](https://img.shields.io/badge/latency-%3C50ms-blue)]()

---

## What Is This?

**FAISS Memory** is a local semantic memory system designed for AI coding assistants. It lets your AI remember context across sessions without expensive cloud vector databases.

**Key Features:**
- üöÄ **Blazing Fast** - <50ms search latency (in-memory FAISS)
- üí∞ **Zero Cost** - 100% local embeddings (no API calls)
- üîí **Private** - Your data never leaves your machine
- üîå **Universal** - Works with Claude Code, Cursor, ChatGPT, Continue.dev, Aider, etc.
- ü§ñ **AI-Ready** - REST API designed for AI tool calling
- üíæ **Auto-Backup** - Automatic backups on every write
- üì¶ **Docker** - One-command deployment

---

## Quick Start

### 1. Run the Service

```bash
# Clone and build
git clone <repo-url> faiss-memory
cd faiss-memory
docker compose up -d

# Verify it's running
curl http://localhost:8900/health
```

### 2. Add Your First Memory

```bash
curl -X POST http://localhost:8900/memory/add \
  -H "Content-Type: application/json" \
  -d '{
    "text": "FAISS Memory is a local semantic search system",
    "source": "readme.md"
  }'
```

### 3. Search

```bash
curl -X POST http://localhost:8900/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is FAISS Memory?",
    "k": 3
  }' | jq
```

**That's it!** The service is now running at http://localhost:8900

---

## Integration Guides

**Choose your AI coding assistant:**

- üìò [**Claude Code**](integrations/claude-code.md) - Anthropic's official CLI
- ü§ñ [**Codex**](integrations/codex.md) - OpenAI Codex
- üí¨ [**ChatGPT**](integrations/chatgpt.md) - Custom GPT Actions
- üñ±Ô∏è [**Cursor**](integrations/cursor.md) - Cursor AI IDE
- ‚ö° [**Continue.dev**](integrations/continue.md) - VS Code extension
- üõ†Ô∏è [**Aider**](integrations/aider.md) - AI pair programming
- üîå [**Generic**](integrations/generic.md) - Any HTTP-capable AI

Each guide includes:
- ‚úÖ Step-by-step setup
- ‚úÖ Configuration files
- ‚úÖ Example prompts
- ‚úÖ Best practices

---

## Why FAISS Memory?

### The Problem

AI coding assistants are powerful but **forget everything** between sessions:
- Previous conversations? Gone.
- Project decisions? Lost.
- Code patterns you prefer? Vanished.
- Past bugs and fixes? Forgotten.

### Traditional Solutions (Expensive)

- **Pinecone:** $70-200/month for production use
- **Weaviate:** Self-host complexity + infrastructure costs
- **OpenAI Embeddings:** $0.13 per 1M tokens (adds up fast)
- **Chroma/LanceDB:** Better, but still cloud-dependent for embeddings

### FAISS Memory (This Project)

- **$0/month** - 100% local embeddings (sentence-transformers)
- **<50ms queries** - In-memory FAISS index
- **5-minute setup** - Docker Compose, done
- **Works everywhere** - Any AI that can call HTTP APIs
- **Private** - No data sent to cloud services

---

## How It Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Coding Assistant (Claude Code, Cursor, etc) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ HTTP POST /search
                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  FAISS Memory Service ‚îÇ  (Docker :8900)
        ‚îÇ  - FastAPI REST API   ‚îÇ
        ‚îÇ  - Auto-backups       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Memory Engine       ‚îÇ
        ‚îÇ  - FAISS IndexFlatIP  ‚îÇ
        ‚îÇ  - sentence-transformers ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Persistent Storage   ‚îÇ
        ‚îÇ  - index.faiss        ‚îÇ
        ‚îÇ  - metadata.json      ‚îÇ
        ‚îÇ  - backups/           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Step-by-step:**
1. Your AI sends a query: `POST /search {"query": "How do I handle errors?"}`
2. Service generates embeddings using local model (all-MiniLM-L6-v2)
3. FAISS searches in-memory index (<50ms)
4. Returns top-k most similar memories with scores
5. AI uses context to answer your question

**No API calls. No cloud dependencies. Just fast, local semantic search.**

---

## Use Cases

### For AI Coding Assistants

**1. Project Context Persistence**
```bash
# Store project decisions
POST /memory/add
{
  "text": "Use React Query for API calls, not Redux Toolkit Query",
  "source": "architecture-decisions.md"
}

# AI can recall later
POST /search {"query": "What should I use for API calls?"}
# Returns: "Use React Query for API calls..."
```

**2. Code Pattern Memory**
```bash
# Store your preferred patterns
POST /memory/add
{
  "text": "For error handling, use custom Error classes with specific codes",
  "source": "code-standards.md"
}

# AI suggests consistent patterns
POST /search {"query": "How should I handle errors?"}
```

**3. Bug & Fix History**
```bash
# Record bugs and solutions
POST /memory/add
{
  "text": "Fixed: Docker health checks fail with 'localhost' on some systems. Use 127.0.0.1 instead",
  "source": "bug-fixes.md"
}

# Avoid repeating mistakes
POST /search {"query": "Docker health check not working"}
```

**4. Duplicate Detection**
```bash
# Check before adding new memories
POST /memory/is-novel
{
  "text": "We use Tailwind CSS for styling",
  "threshold": 0.85
}
# Returns: {"is_novel": false} (already known)
```

### For Personal Knowledge Management

- üìö **Second Brain** - Store notes, ideas, learnings
- üîç **Smart Search** - Find related concepts semantically
- üìù **Meeting Notes** - Recall decisions from past meetings
- üìñ **Documentation** - Search your docs faster than Ctrl+F

---

## API Reference

### Core Endpoints

#### `POST /search`
Search memories using natural language query.

**Request:**
```json
{
  "query": "How do we handle authentication?",
  "k": 5,
  "threshold": 0.3
}
```

**Response:**
```json
{
  "query": "How do we handle authentication?",
  "results": [
    {
      "id": 42,
      "text": "Use JWT tokens with 15min expiry...",
      "source": "auth-system.md",
      "similarity": 0.89,
      "timestamp": "2026-02-12T10:30:00"
    }
  ],
  "count": 1
}
```

#### `POST /memory/add`
Add a new memory.

**Request:**
```json
{
  "text": "Always use TypeScript strict mode",
  "source": "code-standards.md",
  "metadata": {"type": "guideline", "priority": "high"}
}
```

**Response:**
```json
{
  "success": true,
  "id": 43,
  "message": "Memory added successfully"
}
```

#### `POST /memory/is-novel`
Check if text is novel (not already in memory).

**Request:**
```json
{
  "text": "Use React Query for API calls",
  "threshold": 0.85
}
```

**Response:**
```json
{
  "is_novel": false,
  "closest_match": {
    "id": 12,
    "text": "Use React Query for data fetching",
    "similarity": 0.92
  }
}
```

#### `GET /stats`
Get index statistics.

**Response:**
```json
{
  "total_memories": 156,
  "dimension": 384,
  "model": "all-MiniLM-L6-v2",
  "created_at": "2026-02-12T10:00:00",
  "last_updated": "2026-02-12T18:30:00",
  "index_size_bytes": 234567,
  "backup_count": 10
}
```

**Full API documentation:** [docs/API.md](docs/API.md)

---

## Performance

| Metric | Value | Hardware |
|--------|-------|----------|
| **Search latency** | <50ms | Mac mini M4 Pro |
| **Add latency** | ~100ms | (includes backup) |
| **Model loading** | ~15s | First start only |
| **Memory footprint** | ~200MB | Container |
| **Index size** | ~1.5KB/memory | Linear growth |
| **Throughput** | ~100 searches/sec | Single-threaded |

**Benchmarked with:**
- 156 memories (~250KB total text)
- k=5 results per search
- Mac mini M4 Pro, 16GB RAM

---

## Architecture

### Components

**FastAPI Service** (`app.py`)
- REST API endpoints
- Request validation (Pydantic)
- Health checks
- Error handling

**Memory Engine** (`memory_engine.py`)
- FAISS IndexFlatIP (inner product similarity)
- sentence-transformers (all-MiniLM-L6-v2)
- Auto-backup system (keeps last 10)
- Metadata management (JSON)

**Docker Container**
- Python 3.11 slim base
- Persistent volume for data
- Health checks (curl)
- Graceful shutdown

### Data Storage

```
data/
‚îú‚îÄ‚îÄ index.faiss       # FAISS binary index
‚îú‚îÄ‚îÄ metadata.json     # Memory metadata (source, timestamp, etc.)
‚îî‚îÄ‚îÄ backups/          # Automatic backups
    ‚îú‚îÄ‚îÄ manual_20260212_120000/
    ‚îú‚îÄ‚îÄ auto_20260212_120530/
    ‚îî‚îÄ‚îÄ ... (keeps last 10)
```

**Backup Strategy:**
- Auto-backup on every `add` or `build` operation
- Keeps last 10 backups (auto-cleanup)
- Manual backups via `POST /backup`
- Restore: copy files from backup directory

---

## Configuration

### Environment Variables

```bash
# Port (default: 8000)
PORT=8000

# Model (default: all-MiniLM-L6-v2)
MODEL_NAME=all-MiniLM-L6-v2

# Data directory (default: /data)
DATA_DIR=/data

# Max backups to keep (default: 10)
MAX_BACKUPS=10
```

### Docker Compose

```yaml
services:
  faiss-memory:
    build: .
    image: faiss-memory:latest
    container_name: faiss-memory
    restart: unless-stopped
    ports:
      - "8900:8000"
    volumes:
      - ./data:/data
    environment:
      - MODEL_NAME=all-MiniLM-L6-v2
      - MAX_BACKUPS=10
```

---

## Advanced Usage

### Custom Embeddings Model

Change the model in `memory_engine.py`:

```python
from sentence_transformers import SentenceTransformer

# Default: all-MiniLM-L6-v2 (384 dims, 80MB)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Alternatives:
# - all-mpnet-base-v2 (768 dims, 420MB, more accurate)
# - paraphrase-MiniLM-L3-v2 (384 dims, 60MB, faster)
# - multi-qa-MiniLM-L6-cos-v1 (384 dims, Q&A optimized)
```

### Batch Operations

Add multiple memories at once:

```bash
curl -X POST http://localhost:8900/memory/add-batch \
  -H "Content-Type: application/json" \
  -d '{
    "memories": [
      {"text": "Memory 1", "source": "file1.md"},
      {"text": "Memory 2", "source": "file2.md"}
    ]
  }'
```

### Index Rebuilding

Rebuild the entire index from files:

```bash
curl -X POST http://localhost:8900/index/build \
  -H "Content-Type: application/json" \
  -d '{
    "sources": [
      "/path/to/memory-files/*.md"
    ]
  }'
```

---

## Troubleshooting

### Service won't start

```bash
# Check logs
docker logs faiss-memory

# Common issues:
# 1. Port 8900 in use ‚Üí change port in docker-compose.yml
# 2. Model download failed ‚Üí check internet connection
# 3. Permissions ‚Üí ensure data/ directory is writable
```

### Search returns no results

```bash
# Check if index is built
curl http://localhost:8900/stats | jq '.total_memories'

# If 0, index is empty - add memories or rebuild
```

### Slow searches (>100ms)

```bash
# Check container resources
docker stats faiss-memory

# If CPU/memory constrained:
# 1. Increase Docker resource limits
# 2. Use smaller model (paraphrase-MiniLM-L3-v2)
# 3. Reduce k value (fewer results)
```

### Unhealthy container

```bash
# Check health
docker inspect faiss-memory | grep -A 10 Health

# Manual health check
docker exec faiss-memory curl -f http://localhost:8000/health

# If fails:
# 1. Check port in Dockerfile (should be 8000)
# 2. Verify model loaded (check logs)
# 3. Restart: docker compose restart faiss-memory
```

**More troubleshooting:** [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)

---

## Roadmap

### v1.0 (Current - Feb 2026)
- ‚úÖ Core FAISS semantic search
- ‚úÖ REST API (FastAPI)
- ‚úÖ Auto-backups
- ‚úÖ Docker deployment
- ‚úÖ Basic integrations (Claude Code, Cursor, etc.)

### v1.1 (Q1 2026)
- [ ] Web UI for browsing/managing memories
- [ ] Auto-rebuild on file changes (watch mode)
- [ ] MCP server implementation
- [ ] Memory deduplication tool
- [ ] Export formats (JSON, Markdown, CSV)

### v1.2 (Q2 2026)
- [ ] Multi-index support (separate indexes per project)
- [ ] Hybrid search (semantic + keyword)
- [ ] Memory tagging and filtering
- [ ] Analytics dashboard
- [ ] Scheduled rebuilds via cron
- [ ] Advanced similarity metrics

**Suggest features:** Open an issue!

---

## Contributing

**Status:** Not yet accepting contributions (public release pending)

**When public:**
1. Fork the repo
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

**Guidelines:**
- Follow existing code style (black, flake8)
- Add tests for new features
- Update documentation
- Keep commits atomic and well-described

**See:** [CONTRIBUTING.md](CONTRIBUTING.md) (when available)

---

## License

**To be determined** (pending public release)

Likely: MIT or Apache 2.0

---

## Credits

**Built by:** Jack (OpenClaw AI agent)  
**For:** DK  
**Date:** February 2026

**Technologies:**
- [FAISS](https://github.com/facebookresearch/faiss) - Facebook AI Similarity Search
- [sentence-transformers](https://www.sbert.net/) - UKPLab, Sentence-BERT models
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [Docker](https://www.docker.com/) - Containerization

---

## Support

- üìñ **Documentation:** [docs/](docs/)
- üí° **Examples:** [examples/](examples/)
- üêõ **Issues:** GitHub Issues (when public)
- üí¨ **Discussions:** GitHub Discussions (when public)

---

## Star History

‚≠ê If you find this useful, give it a star!

---

**Built with ‚ù§Ô∏è for the AI coding assistant community**
